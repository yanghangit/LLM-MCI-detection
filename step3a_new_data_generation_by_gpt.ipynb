{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vrH5L15WAooR"
      },
      "outputs": [],
      "source": [
        "PROJECT_HOME = \".\"\n",
        "\n",
        "# # For Colab\n",
        "\n",
        "# PROJECT_HOME = \"/content/drive/My Drive/Projects/LLM-MCI-detection\"\n",
        "\n",
        "# # Google Drive storage setup\n",
        "# from google.colab import drive\n",
        "# drive.mount('/content/drive', force_remount=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3YgQuJKA48hE"
      },
      "outputs": [],
      "source": [
        "%%capture\n",
        "%pip install python-dotenv openai"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nyKH2aNY3lFI"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import openai\n",
        "import pandas as pd\n",
        "from tqdm import tqdm"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Pmj3L7ly5e7z"
      },
      "outputs": [],
      "source": [
        "import dotenv\n",
        "_ = dotenv.load_dotenv(os.path.join(PROJECT_HOME, './secret.env'))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "y1na2AbtB5p7"
      },
      "outputs": [],
      "source": [
        "skip_if_exist = True"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1upnPoX9UOwY"
      },
      "outputs": [],
      "source": [
        "use_azure = True"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "L5xEeULF8gg6"
      },
      "outputs": [],
      "source": [
        "use_gpt4 = False"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "V3riblxDd2q5"
      },
      "outputs": [],
      "source": [
        "if use_azure:\n",
        "    if use_gpt4:\n",
        "        gpt_model_name = \"gpt-4\"\n",
        "    else:\n",
        "        gpt_model_name = \"gpt-35-turbo\"\n",
        "else:\n",
        "    if use_gpt4:\n",
        "        gpt_model_name = \"gpt-4-turbo-preview\"\n",
        "    else:\n",
        "        gpt_model_name = \"gpt-3.5-turbo\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_4rHkl2h6FxO"
      },
      "outputs": [],
      "source": [
        "if use_azure:\n",
        "    client = openai.AzureOpenAI(\n",
        "        api_key=os.getenv(\"AZURE_OPENAI_API_KEY\"),\n",
        "        api_version=\"2023-12-01-preview\",\n",
        "        azure_endpoint=os.getenv(\"AZURE_OPENAI_ENDPOINT\")\n",
        "    )\n",
        "else:\n",
        "    client = openai.OpenAI(api_key=os.getenv('OPENAI_API_KEY'))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tVy4ZRhkeTwW"
      },
      "outputs": [],
      "source": [
        "def request_api(system_message, user_message):\n",
        "    response = client.chat.completions.create(\n",
        "    model=gpt_model_name,\n",
        "    messages=[\n",
        "        {\"role\": \"system\", \"content\": system_message},\n",
        "        {\"role\": \"user\", \"content\": user_message}]\n",
        "    )\n",
        "    text_content = response.choices[0].message.content\n",
        "    return text_content"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "X0tbQaI34IUX"
      },
      "outputs": [],
      "source": [
        "def extract_row_data(row):\n",
        "\n",
        "    label = row['label']\n",
        "    age = row['age']\n",
        "    gender = row['gender']\n",
        "    race = row['race']\n",
        "    education = row['education']\n",
        "    MMSE = row['MMSE']\n",
        "    text = row['text']\n",
        "\n",
        "    if pd.isnull(age):\n",
        "        age = \"MISSING\"\n",
        "    else:\n",
        "        age = int(age)\n",
        "\n",
        "    if gender == 1:\n",
        "        gender == \"male\"\n",
        "    else:\n",
        "        gender = \"female\"\n",
        "\n",
        "    if race == 1:\n",
        "        race = \"White\"\n",
        "    else:\n",
        "        race = \"Non-White\"\n",
        "\n",
        "    if pd.isnull(education):\n",
        "        education = \"MISSING\"\n",
        "    else:\n",
        "        education = int(education)\n",
        "\n",
        "    if pd.isnull(MMSE):\n",
        "        MMSE = \"MISSING\"\n",
        "    else:\n",
        "        MMSE = int(MMSE)\n",
        "\n",
        "    return label, age, gender, race, education, MMSE, text"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iTIql_kku7T8"
      },
      "outputs": [],
      "source": [
        "def get_data_generation_prompt(row, generation_type):\n",
        "\n",
        "    label, age, gender, race, education, MMSE, text = extract_row_data(row)\n",
        "\n",
        "    if generation_type == \"observational\":\n",
        "        assert label == \"MCI\"\n",
        "    elif generation_type == \"cross-lingual\":\n",
        "        assert label == \"MCI\"\n",
        "    elif generation_type == \"counterfactual\":\n",
        "        assert label in [\"NC\", \"MCI\"]\n",
        "        if label == \"NC\":\n",
        "            counterfactual_label = \"MCI\"\n",
        "        else:\n",
        "            counterfactual_label = \"NC\"\n",
        "    else:\n",
        "        raise ValueError(\"Unknown generation type: %s\" % generation_type)\n",
        "\n",
        "    system_message = \"Use the following step-by-step instructions to respond to user inputs. The user inputs are related to the transcription of one test subject labelled %s \" % label\n",
        "    system_message += \"describing the Cookie Theft picture from the Boston Diagnostic Aphasia Exam.\"\n",
        "    system_message += \"Other information of the test subject is provided, including, age, gender, race, education level (number of years), and Mini Mental State Examination (MMSE) score.\"\n",
        "    system_message += \"Before the step-by-step instructions, some background information is listed as follows.\"\n",
        "    system_message += \"This Cookie Theft picture description task is used to determine whether one is probable Alzheimer's disease (AD), mild cognitive impairment (MCI), or normal control (NC).\"\n",
        "    system_message += \"The MMSE score measures one's cognitive function but needs adjustment for the education level.\"\n",
        "    system_message += \"The step-by-step instructions are listed as follows.\"\n",
        "    system_message += \"Step 1 - Explain the characteristics of this text and the reasons behind why this test subject is labelled %s.\" % label\n",
        "\n",
        "    if generation_type in [\"observational\", \"cross-lingual\"]:\n",
        "        system_message += \"Step 2 - Given the explanations from Step 1, rephrase the original transcription to a similar but new transcription in two lines:\"\n",
        "    elif generation_type == \"counterfactual\":\n",
        "        system_message += \"Step 2 - Given the explanations from Step 1, imagine what characteristics a subject labelled with %s would have, \" % counterfactual_label\n",
        "        system_message += \"while keeping the subject's age, gender, race, and education information unchanged.\"\n",
        "        system_message += \"Step 3 - Given the reasons from Step 2, write a new counterfactual transcription labelled with %s in two lines:\" % counterfactual_label\n",
        "\n",
        "    system_message += \"the first line only outputs the new transcription in no more than 150 words, with a prefix 'Text:'; \"\n",
        "    system_message += \"the second line outputs the explanations, with a prefix 'Explanations:'.\"\n",
        "\n",
        "    if generation_type == \"cross-lingual\":\n",
        "        system_message += \"Step 3 - Given Step 2, only translate the text but not explanations into Chinese, with a prefix 'Chinese:'.\"\n",
        "\n",
        "    user_message = \"The original transcription of the test subject is given as follows: %s.\" % text\n",
        "    user_message += \"The label of this transcription is: %s.\" % label\n",
        "    user_message += \"The test subject's age is %s, gender is %s, race is %s, education level (number of years) is %s, and MMSE score is %s.\" % (age, gender, race, education, MMSE)\n",
        "\n",
        "    return system_message, user_message"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kBIniLhTLkqF"
      },
      "source": [
        "# New MCI samples generated by existing MCI samples"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7CGpki3f3t_W"
      },
      "outputs": [],
      "source": [
        "data = pd.read_csv(os.path.join(PROJECT_HOME, 'data', 'original.csv'))\n",
        "observed_data = data[data['label']=='MCI']\n",
        "N_fold_observational_generation = 5\n",
        "\n",
        "for run_number in range(N_fold_observational_generation):\n",
        "    output_dir_name = os.path.join(PROJECT_HOME, 'data', 'observational-generation', '%d' % run_number, gpt_model_name)\n",
        "    os.makedirs(output_dir_name, exist_ok=True)\n",
        "\n",
        "    for idx, original_row in tqdm(observed_data.iterrows(), total=len(observed_data), desc=\"Observational generation run %d\" % run_number):\n",
        "\n",
        "        if skip_if_exist:\n",
        "            if os.path.exists(os.path.join(output_dir_name, f'{idx}.txt')):\n",
        "                continue\n",
        "\n",
        "        system_message, user_message = get_data_generation_prompt(original_row, \"observational\")\n",
        "\n",
        "        try:\n",
        "            text_content = request_api(system_message, user_message)\n",
        "            with open(os.path.join(output_dir_name, f'{idx}.txt'), 'w') as f:\n",
        "                f.write(text_content)\n",
        "        except:\n",
        "            continue"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BhKusIlhLrc7"
      },
      "source": [
        "# New Chinese MCI samples generated by existing English MCI samples"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MNYjMT6tLwiB"
      },
      "outputs": [],
      "source": [
        "data = pd.read_csv(os.path.join(PROJECT_HOME, 'data', 'original.csv'))\n",
        "observed_data = data[data['label']=='MCI']\n",
        "N_fold_cross_lingual_generation = 5\n",
        "\n",
        "for run_number in range(N_fold_cross_lingual_generation):\n",
        "    output_dir_name = os.path.join(PROJECT_HOME, 'data', 'cross-lingual-generation', '%d' % run_number, gpt_model_name)\n",
        "    os.makedirs(output_dir_name, exist_ok=True)\n",
        "\n",
        "    for idx, original_row in tqdm(observed_data.iterrows(), total=len(observed_data), desc=\"Cross-lingual generation run %d\" % run_number):\n",
        "\n",
        "        if skip_if_exist:\n",
        "            if os.path.exists(os.path.join(output_dir_name, f'{idx}.txt')):\n",
        "                continue\n",
        "\n",
        "        system_message, user_message = get_data_generation_prompt(original_row, \"cross-lingual\")\n",
        "\n",
        "        try:\n",
        "            text_content = request_api(system_message, user_message)\n",
        "            with open(os.path.join(output_dir_name, f'{idx}.txt'), 'w') as f:\n",
        "                f.write(text_content)\n",
        "        except:\n",
        "            continue"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NQH7JXJdLw2I"
      },
      "source": [
        "# New counterfactual samples generated by existing observational samples"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zarmqxKs3dhB"
      },
      "outputs": [],
      "source": [
        "data = pd.read_csv(os.path.join(PROJECT_HOME, 'data', 'original.csv'))\n",
        "observed_data = data[data['label']=='NC']\n",
        "output_dir_name = os.path.join(PROJECT_HOME, 'data', 'counterfactual-generation', gpt_model_name)\n",
        "os.makedirs(output_dir_name, exist_ok=True)\n",
        "\n",
        "for idx, original_row in tqdm(observed_data.iterrows(), total=len(observed_data), desc=\"Counterfactual generation\"):\n",
        "\n",
        "    if skip_if_exist:\n",
        "        if os.path.exists(os.path.join(output_dir_name, f'{idx}.txt')):\n",
        "            continue\n",
        "\n",
        "    system_message, user_message = get_data_generation_prompt(original_row, \"counterfactual\")\n",
        "\n",
        "    try:\n",
        "        text_content = request_api(system_message, user_message)\n",
        "        with open(os.path.join(output_dir_name, f'{idx}.txt'), 'w') as f:\n",
        "            f.write(text_content)\n",
        "    except:\n",
        "        continue"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "machine_shape": "hm",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
