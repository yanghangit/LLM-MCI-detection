{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vrH5L15WAooR"
      },
      "outputs": [],
      "source": [
        "PROJECT_HOME = \".\"\n",
        "\n",
        "# # For Colab\n",
        "\n",
        "# PROJECT_HOME = \"/content/drive/My Drive/Projects/LLM-MCI-detection\"\n",
        "\n",
        "# # Google Drive storage setup\n",
        "# from google.colab import drive\n",
        "# drive.mount('/content/drive', force_remount=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3YgQuJKA48hE"
      },
      "outputs": [],
      "source": [
        "%%capture\n",
        "!pip install unsloth\n",
        "# Also get the latest nightly Unsloth!\n",
        "!pip uninstall unsloth -y && pip install --upgrade --no-cache-dir --no-deps git+https://github.com/unslothai/unsloth.git"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VIC2Np98F1Iq"
      },
      "outputs": [],
      "source": [
        "%pip install python-dotenv >/dev/null 2>&1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "m_TCLSZ0F1Iq"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import re\n",
        "\n",
        "import pandas as pd\n",
        "from tqdm import tqdm\n",
        "\n",
        "from unsloth import FastLanguageModel\n",
        "from unsloth.chat_templates import get_chat_template"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "P7iUQv1CF1Iq"
      },
      "outputs": [],
      "source": [
        "import dotenv\n",
        "_ = dotenv.load_dotenv(os.path.join(PROJECT_HOME, './secret.env'))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7IKfQX6bF1Iq"
      },
      "outputs": [],
      "source": [
        "skip_if_exist = True"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZsB-4sNTF1Iq"
      },
      "outputs": [],
      "source": [
        "llm_name = \"gemma-2-9B\"\n",
        "\n",
        "if llm_name == \"llama-3.1-8B\":\n",
        "    model_name = \"unsloth/Meta-Llama-3.1-8B-Instruct\"\n",
        "elif llm_name == \"gemma-2-9B\":\n",
        "    model_name = \"unsloth/gemma-2-9b-it\"\n",
        "else:\n",
        "    raise ValueError(f\"Unknown LLM name: {llm_name}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nyKH2aNY3lFI"
      },
      "outputs": [],
      "source": [
        "# Based on the examples from https://github.com/unslothai/unsloth (Apache-2.0 license)\n",
        "\n",
        "max_seq_length = 1024 * 16 # Choose any! We auto support RoPE Scaling internally!\n",
        "dtype = None # None for auto detection. Float16 for Tesla T4, V100, Bfloat16 for Ampere+\n",
        "load_in_4bit = True # Use 4bit quantization to reduce memory usage. Can be False.\n",
        "\n",
        "model, tokenizer = FastLanguageModel.from_pretrained(\n",
        "    model_name=model_name,\n",
        "    max_seq_length=max_seq_length,\n",
        "    dtype=dtype,\n",
        "    load_in_4bit=load_in_4bit,\n",
        "    token=os.getenv(\"HF_TOKEN\")  # HuggingFace token to access gated models\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hzLAzAOmF1Iq"
      },
      "outputs": [],
      "source": [
        "if llm_name == \"llama-3.1-8B\":\n",
        "    tokenizer = get_chat_template(\n",
        "        tokenizer,\n",
        "        chat_template = \"llama-3.1\",\n",
        "    )\n",
        "elif llm_name == \"gemma-2-9B\":\n",
        "    tokenizer = get_chat_template(\n",
        "        tokenizer,\n",
        "        chat_template = \"gemma\",\n",
        "    )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mAorAzIv9-na"
      },
      "outputs": [],
      "source": [
        "FastLanguageModel.for_inference(model) # Enable native 2x faster inference"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9hmerb5XF1Ir"
      },
      "outputs": [],
      "source": [
        "def request_api(system_message, user_message):\n",
        "\n",
        "    messages = [\n",
        "        {\"role\": \"system\", \"content\": system_message},\n",
        "        {\"role\": \"user\", \"content\": user_message}\n",
        "    ]\n",
        "\n",
        "    if llm_name == \"llama-3.1-8B\":\n",
        "\n",
        "        inputs = tokenizer.apply_chat_template(\n",
        "            messages,\n",
        "            tokenize=True,\n",
        "            add_generation_prompt=True,  # Must add for generation\n",
        "            return_tensors=\"pt\",\n",
        "        ).to(\"cuda\")\n",
        "\n",
        "        outputs = model.generate(input_ids=inputs, max_new_tokens=1024*16, use_cache=True,\n",
        "                                 temperature=1.5, min_p=0.1) # configuration from https://arxiv.org/abs/2407.01082\n",
        "\n",
        "    elif llm_name == \"gemma-2-9B\":\n",
        "\n",
        "        inputs = tokenizer.apply_chat_template(\n",
        "            messages,\n",
        "            tokenize=True,\n",
        "            add_generation_prompt=True,  # Must add for generation\n",
        "            return_tensors=\"pt\",\n",
        "        ).to(\"cuda\")\n",
        "\n",
        "        outputs = model.generate(input_ids=inputs, max_new_tokens=1024*4, use_cache=True,\n",
        "                                 temperature=1.5, min_p=0.1)\n",
        "\n",
        "    text = tokenizer.batch_decode(outputs)\n",
        "\n",
        "    if llm_name == \"llama-3.1-8B\":\n",
        "        text = ''.join(text)\n",
        "        matched_text = re.search(r'<\\|start_header_id\\|>assistant<\\|end_header_id\\|>(.*?)<\\|eot_id\\|>', text, re.DOTALL)\n",
        "        if matched_text:\n",
        "            text_content = matched_text.group(1).strip()\n",
        "        else:\n",
        "            text_content = ''\n",
        "    elif llm_name == \"gemma-2-9B\":\n",
        "        text = ''.join(text)\n",
        "        text_content = re.search(r'<start_of_turn>model(.*?)<end_of_turn>', text, re.DOTALL).group(1).strip()\n",
        "\n",
        "    return text_content"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kELYfYs_F1Ir"
      },
      "outputs": [],
      "source": [
        "def extract_row_data(row):\n",
        "\n",
        "    label = row['label']\n",
        "    age = row['age']\n",
        "    gender = row['gender']\n",
        "    race = row['race']\n",
        "    education = row['education']\n",
        "    MMSE = row['MMSE']\n",
        "    text = row['text']\n",
        "\n",
        "    if pd.isnull(age):\n",
        "        age = \"MISSING\"\n",
        "    else:\n",
        "        age = int(age)\n",
        "\n",
        "    if gender == 1:\n",
        "        gender == \"male\"\n",
        "    else:\n",
        "        gender = \"female\"\n",
        "\n",
        "    if race == 1:\n",
        "        race = \"White\"\n",
        "    else:\n",
        "        race = \"Non-White\"\n",
        "\n",
        "    if pd.isnull(education):\n",
        "        education = \"MISSING\"\n",
        "    else:\n",
        "        education = int(education)\n",
        "\n",
        "    if pd.isnull(MMSE):\n",
        "        MMSE = \"MISSING\"\n",
        "    else:\n",
        "        MMSE = int(MMSE)\n",
        "\n",
        "    return label, age, gender, race, education, MMSE, text"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YacZZm9iF1Ir"
      },
      "outputs": [],
      "source": [
        "def get_data_generation_prompt(row, generation_type):\n",
        "\n",
        "    label, age, gender, race, education, MMSE, text = extract_row_data(row)\n",
        "\n",
        "    if generation_type == \"observational\":\n",
        "        assert label == \"MCI\"\n",
        "    elif generation_type == \"cross-lingual\":\n",
        "        assert label == \"MCI\"\n",
        "    elif generation_type == \"counterfactual\":\n",
        "        assert label in [\"NC\", \"MCI\"]\n",
        "        if label == \"NC\":\n",
        "            counterfactual_label = \"MCI\"\n",
        "        else:\n",
        "            counterfactual_label = \"NC\"\n",
        "    else:\n",
        "        raise ValueError(\"Unknown generation type: %s\" % generation_type)\n",
        "\n",
        "    system_message = \"Use the following step-by-step instructions to respond to user inputs. The user inputs are related to the transcription of one test subject labelled %s \" % label\n",
        "    system_message += \"describing the Cookie Theft picture from the Boston Diagnostic Aphasia Exam.\"\n",
        "    system_message += \"Other information of the test subject is provided, including, age, gender, race, education level (number of years), and Mini Mental State Examination (MMSE) score.\"\n",
        "    system_message += \"Before the step-by-step instructions, some background information is listed as follows.\"\n",
        "    system_message += \"This Cookie Theft picture description task is used to determine whether one is probable Alzheimer's disease (AD), mild cognitive impairment (MCI), or normal control (NC).\"\n",
        "    system_message += \"The MMSE score measures one's cognitive function but needs adjustment for the education level.\"\n",
        "    system_message += \"The step-by-step instructions are listed as follows.\"\n",
        "    system_message += \"Step 1 - Explain the characteristics of this text and the reasons behind why this test subject is labelled %s.\" % label\n",
        "\n",
        "    if generation_type in [\"observational\", \"cross-lingual\"]:\n",
        "        system_message += \"Step 2 - Given the explanations from Step 1, rephrase the original transcription to a similar but new transcription in two lines:\"\n",
        "    elif generation_type == \"counterfactual\":\n",
        "        system_message += \"Step 2 - Given the explanations from Step 1, imagine what characteristics a subject labelled with %s would have, \" % counterfactual_label\n",
        "        system_message += \"while keeping the subject's age, gender, race, and education information unchanged.\"\n",
        "        system_message += \"Step 3 - Given the reasons from Step 2, write a new counterfactual transcription labelled with %s in two lines:\" % counterfactual_label\n",
        "\n",
        "    system_message += \"the first line only outputs the new transcription in no more than 150 words, with a prefix 'Text:'; \"\n",
        "    system_message += \"the second line outputs the explanations, with a prefix 'Explanations:'.\"\n",
        "\n",
        "    if generation_type == \"cross-lingual\":\n",
        "        system_message += \"Step 3 - Given Step 2, only translate the text but not explanations into Chinese, with a prefix 'Chinese:'.\"\n",
        "\n",
        "    user_message = \"The original transcription of the test subject is given as follows: %s.\" % text\n",
        "    user_message += \"The label of this transcription is: %s.\" % label\n",
        "    user_message += \"The test subject's age is %s, gender is %s, race is %s, education level (number of years) is %s, and MMSE score is %s.\" % (age, gender, race, education, MMSE)\n",
        "\n",
        "    return system_message, user_message"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J2c0IXUYF1Ir"
      },
      "source": [
        "# New MCI samples generated by existing MCI samples"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "faJp6iRGF1Ir"
      },
      "outputs": [],
      "source": [
        "data = pd.read_csv(os.path.join(PROJECT_HOME, 'data', 'original.csv'))\n",
        "observed_data = data[data['label']=='MCI']\n",
        "N_fold_observational_generation = 5\n",
        "\n",
        "for run_number in range(N_fold_observational_generation):\n",
        "    output_dir_name = os.path.join(PROJECT_HOME, 'data', 'observational-generation', '%d' % run_number, llm_name)\n",
        "    os.makedirs(output_dir_name, exist_ok=True)\n",
        "\n",
        "    for idx, original_row in tqdm(observed_data.iterrows(), total=len(observed_data), desc=\"Observational generation run %d\" % run_number):\n",
        "\n",
        "        if skip_if_exist:\n",
        "            if os.path.exists(os.path.join(output_dir_name, f'{idx}.txt')):\n",
        "                continue\n",
        "\n",
        "        system_message, user_message = get_data_generation_prompt(original_row, \"observational\")\n",
        "\n",
        "        try:\n",
        "            text_content = request_api(system_message, user_message)\n",
        "            with open(os.path.join(output_dir_name, f'{idx}.txt'), 'w') as f:\n",
        "                f.write(text_content)\n",
        "        except:\n",
        "            continue"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pSP91nVdF1Ir"
      },
      "source": [
        "# New Chinese MCI samples generated by existing English MCI samples"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GFaj7Mx1F1Is"
      },
      "outputs": [],
      "source": [
        "data = pd.read_csv(os.path.join(PROJECT_HOME, 'data', 'original.csv'))\n",
        "observed_data = data[data['label']=='MCI']\n",
        "N_fold_cross_lingual_generation = 5\n",
        "\n",
        "for run_number in range(N_fold_cross_lingual_generation):\n",
        "    output_dir_name = os.path.join(PROJECT_HOME, 'data', 'cross-lingual-generation', '%d' % run_number, llm_name)\n",
        "    os.makedirs(output_dir_name, exist_ok=True)\n",
        "\n",
        "    for idx, original_row in tqdm(observed_data.iterrows(), total=len(observed_data), desc=\"Cross-lingual generation run %d\" % run_number):\n",
        "\n",
        "        if skip_if_exist:\n",
        "            if os.path.exists(os.path.join(output_dir_name, f'{idx}.txt')):\n",
        "                continue\n",
        "\n",
        "        system_message, user_message = get_data_generation_prompt(original_row, \"cross-lingual\")\n",
        "\n",
        "        try:\n",
        "            text_content = request_api(system_message, user_message)\n",
        "            with open(os.path.join(output_dir_name, f'{idx}.txt'), 'w') as f:\n",
        "                f.write(text_content)\n",
        "        except:\n",
        "            continue"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Nm51NcmVF1Is"
      },
      "source": [
        "# New counterfactual samples generated by existing observational samples"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "a5n5oGb8F1Is"
      },
      "outputs": [],
      "source": [
        "data = pd.read_csv(os.path.join(PROJECT_HOME, 'data', 'original.csv'))\n",
        "observed_data = data[data['label']=='NC']\n",
        "output_dir_name = os.path.join(PROJECT_HOME, 'data', 'counterfactual-generation', llm_name)\n",
        "os.makedirs(output_dir_name, exist_ok=True)\n",
        "\n",
        "for idx, original_row in tqdm(observed_data.iterrows(), total=len(observed_data), desc=\"Counterfactual generation\"):\n",
        "\n",
        "    if skip_if_exist:\n",
        "        if os.path.exists(os.path.join(output_dir_name, f'{idx}.txt')):\n",
        "            continue\n",
        "\n",
        "    system_message, user_message = get_data_generation_prompt(original_row, \"counterfactual\")\n",
        "\n",
        "    try:\n",
        "        text_content = request_api(system_message, user_message)\n",
        "        with open(os.path.join(output_dir_name, f'{idx}.txt'), 'w') as f:\n",
        "            f.write(text_content)\n",
        "    except:\n",
        "        continue"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "machine_shape": "hm",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
